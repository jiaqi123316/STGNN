{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stgraph_trainer.datasets import preprocess_data_for_lstm_model\n",
    "from stgraph_trainer.datasets import inverse_diff\n",
    "from stgraph_trainer.utils import get_config_from_json\n",
    "from stgraph_trainer.utils import compute_metrics\n",
    "from stgraph_trainer.utils import save_predictions\n",
    "from stgraph_trainer.utils import save_metrics\n",
    "from stgraph_trainer.models import create_lstm_model\n",
    "from stgraph_trainer.trainers import LSTMTrainer\n",
    "from stgraph_trainer.callbacks import PostPredictionCallback\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.12.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.2-cp312-cp312-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m135.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m146.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.0 scikit-learn-1.4.2 threadpoolctl-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from stgraph_trainer.datasets import load_province_temporal_data\n",
    "from stgraph_trainer.datasets import data_diff\n",
    "from stgraph_trainer.datasets import timeseries_to_supervised\n",
    "from stgraph_trainer.datasets import preprocess_data_for_lstm_model\n",
    "from stgraph_trainer.datasets import inverse_diff\n",
    "from stgraph_trainer.utils import get_config_from_json\n",
    "from stgraph_trainer.utils import compute_metrics\n",
    "from stgraph_trainer.utils import save_predictions\n",
    "from stgraph_trainer.utils import save_metrics\n",
    "from stgraph_trainer.models import create_lstm_model\n",
    "from stgraph_trainer.trainers import LSTMTrainer\n",
    "from stgraph_trainer.callbacks import PostPredictionCallback\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up configs and parameters\n",
    "data_config_file = os.getcwd() + '/configs/data_config.json'\n",
    "lstm_config_file = os.getcwd() + '/configs/lstm_config.json'\n",
    "model_name = 'lstm'\n",
    "work_dir = os.getcwd() + '/results'\n",
    "\n",
    "data_configs = get_config_from_json(data_config_file)\n",
    "lstm_configs = get_config_from_json(lstm_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = get_config_from_json(data_config_file)\n",
    "lstm_configs = get_config_from_json(lstm_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROVINCES = data_configs['provinces']\n",
    "SPLIT_DATE = data_configs['split_date']\n",
    "TIME_STEPS = int(data_configs['time_steps'])\n",
    "STATUS = data_configs['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_UNITS = int(lstm_configs['lstm_units'])\n",
    "BATCH_SIZE = int(lstm_configs['batch_size'])\n",
    "DROP_RATE = float(lstm_configs['drop_rate'])\n",
    "RECURRENT_DROP_RATE = float(lstm_configs['recurrent_drop_rate'])\n",
    "TRIALS = int(lstm_configs['trials'])\n",
    "EPOCHS = int(lstm_configs['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "df = pd.read_csv('../notebooks/stgraph_trainer/datasets/data/water_nitrite (1).csv', index_col=0)\n",
    "df = df.set_index('dateTime')\n",
    "\n",
    "\n",
    "#df = load_province_temporal_data(provinces=PROVINCES, status=STATUS)\n",
    "#df = pd.read_csv('../notebooks/stgraph_trainer/datasets/data/temporal_data.csv')\n",
    "#df = df.set_index('datetime')\n",
    "X_train, y_train, X_test, y_test, _, _, scaler = preprocess_data_for_lstm_model(df,\n",
    "                                                                                SPLIT_DATE,\n",
    "                                                                                TIME_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267, 7, 24) (267, 24) (91, 7, 24) (91, 24)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_features = X_train.shape[-1]\n",
    "n_test_samples = len(y_test)\n",
    "output_size = X_train.shape[-1]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Functions to han"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to handle post-prediction\n",
    "# including inverse transform and add back difference\n",
    "# to make predictions back to original scale.\n",
    "def inverse_transform(x, scaler, **kwargs):\n",
    "  return scaler.inverse_transform(x)\n",
    "\n",
    "inv_trans = partial(inverse_transform, scaler=scaler)\n",
    "inv_trans._order = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_diff_1(x, raw_values, n_test=None, idx=0):\n",
    "  return inverse_diff(x, raw_values, n_test + 1 - idx)\n",
    "inv_diff = partial(inv_diff_1, raw_values=df.values, n_test=n_test_samples)\n",
    "inv_diff._order = 20\n",
    "\n",
    "tfms = [inv_trans, inv_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to LSTM: {'batch_shape': (1, 7, 24)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jiaqi/STGNN/notebooks/lstm.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(TRIALS):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   model \u001b[39m=\u001b[39m create_lstm_model(LSTM_UNITS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                             output_size,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                             BATCH_SIZE,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                             TIME_STEPS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                             n_features,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                             DROP_RATE,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                             RECURRENT_DROP_RATE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n",
      "File \u001b[0;32m~/STGNN/notebooks/stgraph_trainer/models/lstm_model.py:41\u001b[0m, in \u001b[0;36mcreate_lstm_model\u001b[0;34m(lstm_units, output_size, batch_size, time_steps, n_features, dropout, recurrent_dropout)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_lstm_model\u001b[39m(lstm_units,\n\u001b[1;32m      4\u001b[0m                       output_size,\n\u001b[1;32m      5\u001b[0m                       batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                       dropout\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m,\n\u001b[1;32m      9\u001b[0m                       recurrent_dropout\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m):\n\u001b[1;32m     10\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m  Create a LSTM model.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m  A LSTM model, an instance of tf.keras.Sequential.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m---> 41\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mLSTM(units\u001b[39m=\u001b[39;49mlstm_units,\n\u001b[1;32m     42\u001b[0m                          stateful\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     43\u001b[0m                          batch_shape\u001b[39m=\u001b[39;49m(batch_size, time_steps, n_features),\n\u001b[1;32m     44\u001b[0m                          dropout\u001b[39m=\u001b[39;49mdropout,\n\u001b[1;32m     45\u001b[0m                          recurrent_dropout\u001b[39m=\u001b[39;49mrecurrent_dropout,\n\u001b[1;32m     46\u001b[0m                          return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     47\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(dropout),\n\u001b[1;32m     48\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mLSTM(units\u001b[39m=\u001b[39mlstm_units,\n\u001b[1;32m     49\u001b[0m                          dropout\u001b[39m=\u001b[39mdropout,\n\u001b[1;32m     50\u001b[0m                          recurrent_dropout\u001b[39m=\u001b[39mrecurrent_dropout,\n\u001b[1;32m     51\u001b[0m                          stateful\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     52\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(dropout),\n\u001b[1;32m     53\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(output_size)\n\u001b[1;32m     54\u001b[0m   ])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/lstm.py:483\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, seed, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    434\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    435\u001b[0m     units,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    459\u001b[0m ):\n\u001b[1;32m    460\u001b[0m     cell \u001b[39m=\u001b[39m LSTMCell(\n\u001b[1;32m    461\u001b[0m         units,\n\u001b[1;32m    462\u001b[0m         activation\u001b[39m=\u001b[39mactivation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m         implementation\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m),\n\u001b[1;32m    482\u001b[0m     )\n\u001b[0;32m--> 483\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    484\u001b[0m         cell,\n\u001b[1;32m    485\u001b[0m         return_sequences\u001b[39m=\u001b[39;49mreturn_sequences,\n\u001b[1;32m    486\u001b[0m         return_state\u001b[39m=\u001b[39;49mreturn_state,\n\u001b[1;32m    487\u001b[0m         go_backwards\u001b[39m=\u001b[39;49mgo_backwards,\n\u001b[1;32m    488\u001b[0m         stateful\u001b[39m=\u001b[39;49mstateful,\n\u001b[1;32m    489\u001b[0m         unroll\u001b[39m=\u001b[39;49munroll,\n\u001b[1;32m    490\u001b[0m         activity_regularizer\u001b[39m=\u001b[39;49mactivity_regularizer,\n\u001b[1;32m    491\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m InputSpec(ndim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m    494\u001b[0m     \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39mbackend() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m backend\u001b[39m.\u001b[39mcudnn_ok(\n\u001b[1;32m    495\u001b[0m         cell\u001b[39m.\u001b[39mactivation,\n\u001b[1;32m    496\u001b[0m         cell\u001b[39m.\u001b[39mrecurrent_activation,\n\u001b[1;32m    497\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munroll,\n\u001b[1;32m    498\u001b[0m         cell\u001b[39m.\u001b[39muse_bias,\n\u001b[1;32m    499\u001b[0m     ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:205\u001b[0m, in \u001b[0;36mRNN.__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, zero_output_for_mask, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstate_size\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(cell):\n\u001b[1;32m    199\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    200\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe RNN cell should have a `state_size` attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(single integer or list of integers, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mone integer per RNN state). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived: cell=\u001b[39m\u001b[39m{\u001b[39;00mcell\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    207\u001b[0m \u001b[39m# If True, the output for masked timestep will be zeros, whereas in the\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m# False case, output from previous timestep is returned for masked\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m# timestep.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_output_for_mask \u001b[39m=\u001b[39m zero_output_for_mask\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/layer.py:265\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_shape_arg \u001b[39m=\u001b[39m input_shape_arg\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnrecognized keyword arguments \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpassed to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_policy \u001b[39m=\u001b[39m dtype_policies\u001b[39m.\u001b[39mget(dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to LSTM: {'batch_shape': (1, 7, 24)}"
     ]
    }
   ],
   "source": [
    "for trial in range(TRIALS):\n",
    "  model = create_lstm_model(LSTM_UNITS,\n",
    "                            output_size,\n",
    "                            BATCH_SIZE,\n",
    "                            TIME_STEPS,\n",
    "                            n_features,\n",
    "                            DROP_RATE,\n",
    "                            RECURRENT_DROP_RATE)\n",
    "  print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to LSTM: {'batch_input_shape': (1, 7, 24)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jiaqi/STGNN/notebooks/lstm.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mae_results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(TRIALS):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   model \u001b[39m=\u001b[39m create_lstm_model(LSTM_UNITS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                             output_size,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                             BATCH_SIZE,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                             TIME_STEPS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                             n_features,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                             DROP_RATE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                             RECURRENT_DROP_RATE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaqi/STGNN/notebooks/lstm.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   train_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((X_train, y_train))\u001b[39m.\u001b[39mbatch(BATCH_SIZE)\n",
      "File \u001b[0;32m~/STGNN/notebooks/stgraph_trainer/models/lstm_model.py:41\u001b[0m, in \u001b[0;36mcreate_lstm_model\u001b[0;34m(lstm_units, output_size, batch_size, time_steps, n_features, dropout, recurrent_dropout)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_lstm_model\u001b[39m(lstm_units,\n\u001b[1;32m      4\u001b[0m                       output_size,\n\u001b[1;32m      5\u001b[0m                       batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                       dropout\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m,\n\u001b[1;32m      9\u001b[0m                       recurrent_dropout\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m):\n\u001b[1;32m     10\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m  Create a LSTM model.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m  A LSTM model, an instance of tf.keras.Sequential.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m---> 41\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mLSTM(units\u001b[39m=\u001b[39;49mlstm_units,\n\u001b[1;32m     42\u001b[0m                          stateful\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     43\u001b[0m                          batch_input_shape\u001b[39m=\u001b[39;49m(batch_size, time_steps, n_features),\n\u001b[1;32m     44\u001b[0m                          dropout\u001b[39m=\u001b[39;49mdropout,\n\u001b[1;32m     45\u001b[0m                          recurrent_dropout\u001b[39m=\u001b[39;49mrecurrent_dropout,\n\u001b[1;32m     46\u001b[0m                          return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     47\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(dropout),\n\u001b[1;32m     48\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mLSTM(units\u001b[39m=\u001b[39mlstm_units,\n\u001b[1;32m     49\u001b[0m                          dropout\u001b[39m=\u001b[39mdropout,\n\u001b[1;32m     50\u001b[0m                          recurrent_dropout\u001b[39m=\u001b[39mrecurrent_dropout,\n\u001b[1;32m     51\u001b[0m                          stateful\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     52\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(dropout),\n\u001b[1;32m     53\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(output_size)\n\u001b[1;32m     54\u001b[0m   ])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/lstm.py:483\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, seed, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    434\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    435\u001b[0m     units,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    459\u001b[0m ):\n\u001b[1;32m    460\u001b[0m     cell \u001b[39m=\u001b[39m LSTMCell(\n\u001b[1;32m    461\u001b[0m         units,\n\u001b[1;32m    462\u001b[0m         activation\u001b[39m=\u001b[39mactivation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m         implementation\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m),\n\u001b[1;32m    482\u001b[0m     )\n\u001b[0;32m--> 483\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    484\u001b[0m         cell,\n\u001b[1;32m    485\u001b[0m         return_sequences\u001b[39m=\u001b[39;49mreturn_sequences,\n\u001b[1;32m    486\u001b[0m         return_state\u001b[39m=\u001b[39;49mreturn_state,\n\u001b[1;32m    487\u001b[0m         go_backwards\u001b[39m=\u001b[39;49mgo_backwards,\n\u001b[1;32m    488\u001b[0m         stateful\u001b[39m=\u001b[39;49mstateful,\n\u001b[1;32m    489\u001b[0m         unroll\u001b[39m=\u001b[39;49munroll,\n\u001b[1;32m    490\u001b[0m         activity_regularizer\u001b[39m=\u001b[39;49mactivity_regularizer,\n\u001b[1;32m    491\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m InputSpec(ndim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m    494\u001b[0m     \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39mbackend() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m backend\u001b[39m.\u001b[39mcudnn_ok(\n\u001b[1;32m    495\u001b[0m         cell\u001b[39m.\u001b[39mactivation,\n\u001b[1;32m    496\u001b[0m         cell\u001b[39m.\u001b[39mrecurrent_activation,\n\u001b[1;32m    497\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munroll,\n\u001b[1;32m    498\u001b[0m         cell\u001b[39m.\u001b[39muse_bias,\n\u001b[1;32m    499\u001b[0m     ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:205\u001b[0m, in \u001b[0;36mRNN.__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, zero_output_for_mask, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstate_size\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(cell):\n\u001b[1;32m    199\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    200\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe RNN cell should have a `state_size` attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(single integer or list of integers, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mone integer per RNN state). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived: cell=\u001b[39m\u001b[39m{\u001b[39;00mcell\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    207\u001b[0m \u001b[39m# If True, the output for masked timestep will be zeros, whereas in the\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m# False case, output from previous timestep is returned for masked\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m# timestep.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_output_for_mask \u001b[39m=\u001b[39m zero_output_for_mask\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/layer.py:265\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_shape_arg \u001b[39m=\u001b[39m input_shape_arg\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnrecognized keyword arguments \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpassed to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_policy \u001b[39m=\u001b[39m dtype_policies\u001b[39m.\u001b[39mget(dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to LSTM: {'batch_input_shape': (1, 7, 24)}"
     ]
    }
   ],
   "source": [
    "# Create model, train and evaluate\n",
    "rmse_results = []\n",
    "mae_results = []\n",
    "\n",
    "for trial in range(TRIALS):\n",
    "  model = create_lstm_model(LSTM_UNITS,\n",
    "                            output_size,\n",
    "                            BATCH_SIZE,\n",
    "                            TIME_STEPS,\n",
    "                            n_features,\n",
    "                            DROP_RATE,\n",
    "                            RECURRENT_DROP_RATE)\n",
    "  print(model.summary())\n",
    "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE)\n",
    "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "  loss_func = tf.losses.MeanSquaredError()\n",
    "  optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "  trainer = LSTMTrainer(model,\n",
    "                        train_ds,\n",
    "                        test_ds,\n",
    "                        loss_func,\n",
    "                        optimizer,\n",
    "                        callbacks=[PostPredictionCallback(funcs=tfms)],\n",
    "                        raw_test=df.iloc[-n_test_samples:].values)\n",
    "\n",
    "  history = trainer.train(EPOCHS)\n",
    "  print(history)\n",
    "\n",
    "  predict = trainer.predict()\n",
    "  save_predictions(predict,\n",
    "                   model_name,\n",
    "                   n_exp=trial,\n",
    "                   columns=PROVINCES,\n",
    "                   index=df.iloc[-n_test_samples:].index,\n",
    "                   path=work_dir)\n",
    "  print(predict.shape)\n",
    "\n",
    "  m, m_avg = compute_metrics(df.iloc[-n_test_samples:], predict, metric='rmse')\n",
    "  m = np.append(m, m_avg)\n",
    "  rmse_results.append(m)\n",
    "  print(m, m_avg)\n",
    "  \n",
    "  m, m_avg = compute_metrics(df.iloc[-n_test_samples:], predict, metric='mae')\n",
    "  m = np.append(m, m_avg)\n",
    "  mae_results.append(m)\n",
    "  print(m, m_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow-1.2.1-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.2.1-cp36-cp36m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Requirement 'tensorflow_io_gcs_filesystem-0.23.1-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl' looks like a filename, but the file does not exist\u001b[0m\n",
      "\u001b[31mERROR: tensorflow_io_gcs_filesystem-0.23.1-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl is not a supported wheel on this platform.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --ignore-installed --upgrade tensorflow_io_gcs_filesystem-0.23.1-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/jiaqi/Library/Python/3.12/lib/python/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.0.5)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.26.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: dm-tree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jiaqi/Library/Python/3.12/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
